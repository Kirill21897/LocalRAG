{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bc6b5a",
   "metadata": {},
   "source": [
    "# Тестирование функций и методов\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3528b",
   "metadata": {},
   "source": [
    "### Модуль Ingestion (Docling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779b9c",
   "metadata": {},
   "source": [
    "#### Загрузка и конвертация документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-23 14:02:37,881 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,882 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,893 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,895 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,255 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,256 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,279 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,280 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate file: data\\processed\\sample.md\n",
      "Result_doc: 1. Научные открытия и технологические прорывы\n",
      "\n",
      "В 2023 году учёные из ЦЕРНа подтвердили существование...\n"
     ]
    }
   ],
   "source": [
    "import src.document_loader as document_loader\n",
    "import config.config as cfg\n",
    "\n",
    "def test_load_document():\n",
    "    file_path = str(cfg.RAW_DATA_DIR / \"sample.pdf\")\n",
    "    result = document_loader.load_document(file_path)\n",
    "    print(f\"Result_doc: {result[0:100]}...\")  # Print first 100 characters for brevity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_load_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f7c69",
   "metadata": {},
   "source": [
    "#### Чанкирование по кол-ву симовлов с перекрытием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d48ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл 'sample.md' успешно разбит.\n",
      "Количество чанков: 12\n",
      "Пример первого чанка:\n",
      "'1. Научные открытия и технологические прорывы\\n\\nВ 2023 году учёные из ЦЕРНа подтвердили существование'...\n"
     ]
    }
   ],
   "source": [
    "from src.chunker import chunk_text\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "file_path = PROCESSED_DATA_DIR / \"sample.md\"\n",
    "markdown_content = file_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "CHUNK_SIZE = 500 \n",
    "OVERLAP = 100\n",
    "file_name = file_path.name # Сохраняем имя для Qdrant\n",
    "\n",
    "chunks = chunk_text(markdown_content, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "\n",
    "print(f\"Файл '{file_name}' успешно разбит.\")\n",
    "print(f\"Количество чанков: {len(chunks)}\")\n",
    "print(f\"Пример первого чанка:\\n{repr(chunks[0][:100])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ec59d",
   "metadata": {},
   "source": [
    "#### Сохранение векторизованных данных в Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb057789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерация векторов для 12 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_collection'\n"
     ]
    }
   ],
   "source": [
    "from src.embedder import vectorize_and_upload\n",
    "\n",
    "q_client, embed_model = vectorize_and_upload(chunks, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384e948",
   "metadata": {},
   "source": [
    "### Модули Retrieval и Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424063d",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756d415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 3 кандидатов через векторный поиск.\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval import retrieve\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "candidates = retrieve(user_query, q_client, embed_model, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db76377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-3 кандидата по запросу:\n",
      "1. {'text': 'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', 'score': 0.7725014352780308, 'metadata': {'source': 'sample.md', 'chunk_id': 5}}\n",
      "\n",
      "2. {'text': 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн', 'score': 0.5040614310777544, 'metadata': {'source': 'sample.md', 'chunk_id': 6}}\n",
      "\n",
      "3. {'text': ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'score': 0.3601881203512009, 'metadata': {'source': 'sample.md', 'chunk_id': 8}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Топ-3 кандидата по запросу:\")\n",
    "for i, candidate in enumerate(candidates, start=1):\n",
    "    print(f\"{i}. {candidate}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1950a",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12638658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "\n",
      "--- Релевантный чанк №1 (Score: 8.6397) ---\n",
      "аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут р...\n",
      "\n",
      "--- Релевантный чанк №2 (Score: 8.4987) ---\n",
      " содержащих стереотипы, она может их воспроизводить.\n",
      "\n",
      "В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и ра...\n",
      "\n",
      "--- Релевантный чанк №3 (Score: 7.9926) ---\n",
      "ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от...\n"
     ]
    }
   ],
   "source": [
    "from src.reranker import rerank\n",
    "\n",
    "final_context = rerank(user_query, candidates, top_n=3)\n",
    "\n",
    "# Вывод результата\n",
    "for i, res in enumerate(final_context):\n",
    "    print(f\"\\n--- Релевантный чанк №{i+1} (Score: {res['rerank_score']:.4f}) ---\")\n",
    "    print(res[\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20977c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_context[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f76b69",
   "metadata": {},
   "source": [
    "### Модуль Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfdfcb",
   "metadata": {},
   "source": [
    "#### Генератор с использованием локальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02169b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сгенерированный ответ ---\n",
      "К 2050 году потепление может достичь **2,7°C**, если выбросы парниковых газов не будут радикально сокращены.\n"
     ]
    }
   ],
   "source": [
    "from src.generator import Generator\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "response = generator.generate(user_query, final_context[0][\"text\"])\n",
    "\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196d20",
   "metadata": {},
   "source": [
    "Получение итогового списка контекста модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d4f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context_text = []\n",
    "for i in range(len(final_context)):\n",
    "    final_context_text.append(final_context[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e9c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн']\n"
     ]
    }
   ],
   "source": [
    "print(final_context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52a094",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation (Ragas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c3c57",
   "metadata": {},
   "source": [
    "#### Полный цикл оценки (Full Pipeline Evaluation)\n",
    "Включает в себя подготовку данных, генерацию ответов и расчет метрик: Faithfulness, Answer Relevancy, Context Precision, Context Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c892886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\src\\evaluation\\ragas_eval.py:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas Evaluator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.ragas_eval import RagasEvaluator\n",
    "\n",
    "# Инициализация Ragas Evaluator\n",
    "evaluator = RagasEvaluator()\n",
    "print(\"Ragas Evaluator initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cbc6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск генерации ответов...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Скольки градусам может достичь потепление к 2050 году?\n",
      "A: К 2050 году, если выбросы парниковых газов не будут радикально сокращены, потепление может достичь *...\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Какие алгоритмы DeepMind используются в медицине?\n",
      "A: Алгоритмы DeepMind, упомянутые в контексте, применяются в медицине для диагностики диабетической рет...\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Каков прогноз численности населения Земли на 2086 год?\n",
      "A: Прогноз численности населения Земли на 2086 год, согласно контексту, составляет **10,4 миллиарда чел...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка тестовых данных (Questions & Ground Truths)\n",
    "test_questions = [\n",
    "    \"Скольки градусам может достичь потепление к 2050 году?\",\n",
    "    \"Какие алгоритмы DeepMind используются в медицине?\",\n",
    "    \"Каков прогноз численности населения Земли на 2086 год?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Потепление может достичь 2,7°C.\",\n",
    "    \"Алгоритмы DeepMind помогают диагностировать диабетическую ретинопатию и рак молочной железы.\",\n",
    "    \"По прогнозам ООН, пик будет достигнут около 2086 года на уровне 10,4 миллиарда человек.\"\n",
    "]\n",
    "\n",
    "# 2. Запуск пайплайна (Inference)\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "print(\"Запуск генерации ответов...\")\n",
    "for q in test_questions:\n",
    "    # Retrieve\n",
    "    candidates = retrieve(q, q_client, embed_model, top_k=5)\n",
    "    final_c = rerank(q, candidates, top_n=3)\n",
    "    \n",
    "    # Extract context texts\n",
    "    c_texts = [c[\"text\"] for c in final_c]\n",
    "    combined_c = \"\\n\\n\".join(c_texts)\n",
    "    \n",
    "    # Generate\n",
    "    resp = generator.generate(q, combined_c)\n",
    "    \n",
    "    answers.append(resp)\n",
    "    contexts.append(c_texts)\n",
    "    print(f\"Q: {q}\\nA: {resp[:100]}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74be77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск оценки Ragas через модуль src.evaluation...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [00:41<07:31, 41.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:08<03:06, 26.63s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [04:23<01:50, 36.67s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [05:17<00:00, 26.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты оценки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Скольки градусам может достичь потепление к 20...</td>\n",
       "      <td>[аду МГЭИК (Межправительственной группы экспер...</td>\n",
       "      <td>К 2050 году, если выбросы парниковых газов не ...</td>\n",
       "      <td>Потепление может достичь 2,7°C.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Какие алгоритмы DeepMind используются в медицине?</td>\n",
       "      <td>[ содержащих стереотипы, она может их воспроиз...</td>\n",
       "      <td>Алгоритмы DeepMind, упомянутые в контексте, пр...</td>\n",
       "      <td>Алгоритмы DeepMind помогают диагностировать ди...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Каков прогноз численности населения Земли на 2...</td>\n",
       "      <td>[аду МГЭИК (Межправительственной группы экспер...</td>\n",
       "      <td>Прогноз численности населения Земли на 2086 го...</td>\n",
       "      <td>По прогнозам ООН, пик будет достигнут около 20...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999443</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Скольки градусам может достичь потепление к 20...   \n",
       "1  Какие алгоритмы DeepMind используются в медицине?   \n",
       "2  Каков прогноз численности населения Земли на 2...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [аду МГЭИК (Межправительственной группы экспер...   \n",
       "1  [ содержащих стереотипы, она может их воспроиз...   \n",
       "2  [аду МГЭИК (Межправительственной группы экспер...   \n",
       "\n",
       "                                            response  \\\n",
       "0  К 2050 году, если выбросы парниковых газов не ...   \n",
       "1  Алгоритмы DeepMind, упомянутые в контексте, пр...   \n",
       "2  Прогноз численности населения Земли на 2086 го...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0                    Потепление может достичь 2,7°C.      1.000000   \n",
       "1  Алгоритмы DeepMind помогают диагностировать ди...      1.000000   \n",
       "2  По прогнозам ООН, пик будет достигнут около 20...      0.666667   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.862986           1.000000             1.0  \n",
       "1          0.702630           1.000000             1.0  \n",
       "2          0.999443           0.333333             1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data/evaluation_results/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. Запуск оценки\n",
    "print(\"Запуск оценки Ragas через модуль src.evaluation...\")\n",
    "df_results = evaluator.run_evaluation(\n",
    "    questions=test_questions,\n",
    "    answers=answers,\n",
    "    contexts=contexts,\n",
    "    ground_truths=ground_truths\n",
    ")\n",
    "\n",
    "print(\"\\nРезультаты оценки:\")\n",
    "display(df_results)\n",
    "\n",
    "# Сохранение результатов\n",
    "evaluator.save_results(df_results, \"data/evaluation_results/evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71380f",
   "metadata": {},
   "source": [
    "## Сравнительное тестирование методов чанкирования\n",
    "---\n",
    "В этом разделе мы запускаем полный цикл RAG (Ingestion -> Chunking -> Embedding -> Retrieval -> Reranking -> Generation -> Evaluation) для каждого из 5 реализованных методов чанкирования, чтобы сравнить их эффективность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a33249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты необходимых модулей\n",
    "import pandas as pd\n",
    "from src.chunkers.recursive_chunker import chunk_recursive\n",
    "from src.chunkers.token_chunker import chunk_token\n",
    "from src.chunkers.markdown_chunker import chunk_markdown\n",
    "from src.chunkers.sentence_window_chunker import chunk_sentence_window\n",
    "from src.chunkers.semantic_chunker import chunk_semantic\n",
    "\n",
    "from src.embedder import vectorize_and_upload\n",
    "from src.retrieval import retrieve\n",
    "from src.reranker import rerank\n",
    "from src.generator import Generator\n",
    "from src.evaluation.ragas_eval import RagasEvaluator\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "# Загрузка текста\n",
    "file_path = PROCESSED_DATA_DIR / \"sample.md\"\n",
    "markdown_content = file_path.read_text(encoding=\"utf-8\")\n",
    "print(f\"Текст загружен, длина: {len(markdown_content)} символов\")\n",
    "\n",
    "# Инициализация общих компонентов\n",
    "generator = Generator()\n",
    "evaluator = RagasEvaluator()\n",
    "print(\"Компоненты инициализированы.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37015c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые данные (Вопросы и эталонные ответы)\n",
    "test_questions = [\n",
    "    \"Скольки градусам может достичь потепление к 2050 году?\",\n",
    "    \"Какие алгоритмы DeepMind используются в медицине?\",\n",
    "    \"Каков прогноз численности населения Земли на 2086 год?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Потепление может достичь 2,7°C.\",\n",
    "    \"Алгоритмы DeepMind помогают диагностировать диабетическую ретинопатию и рак молочной железы.\",\n",
    "    \"По прогнозам ООН, пик будет достигнут около 2086 года на уровне 10,4 миллиарда человек.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf153f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_for_method(method_name, chunk_func, text, questions, ground_truths):\n",
    "    print(f\"\\n{'='*20} Testing Method: {method_name} {'='*20}\")\n",
    "    \n",
    "    # 1. Chunking\n",
    "    print(\"1. Chunking...\")\n",
    "    try:\n",
    "        # Некоторые методы могут требовать доп. параметров, но мы используем дефолтные для теста\n",
    "        chunks = chunk_func(text)\n",
    "        print(f\"   Получено {len(chunks)} чанков.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Ошибка при чанкировании: {e}\")\n",
    "        return None\n",
    "        \n",
    "    if not chunks:\n",
    "        print(\"   Чанки не созданы.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Embedding & Upload (Unique Collection)\n",
    "    collection_name = f\"docs_{method_name.lower().replace(' ', '_')}\"\n",
    "    print(f\"2. Embedding to collection '{collection_name}'...\")\n",
    "    client, embed_model = vectorize_and_upload(chunks, \"sample.md\", collection_name=collection_name)\n",
    "\n",
    "    # 3. Retrieval, Reranking, Generation\n",
    "    print(\"3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\")\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # Retrieve\n",
    "        candidates = retrieve(q, client, embed_model, top_k=5, collection_name=collection_name)\n",
    "        # Rerank\n",
    "        final_c = rerank(q, candidates, top_n=3)\n",
    "        \n",
    "        # Extract context texts\n",
    "        c_texts = [c[\"text\"] for c in final_c]\n",
    "        combined_c = \"\\n\\n\".join(c_texts)\n",
    "        \n",
    "        # Generate\n",
    "        resp = generator.generate(q, combined_c)\n",
    "        answers.append(resp)\n",
    "        contexts.append(c_texts)\n",
    "        # print(f\"   Q: {q[:30]}... -> A: {resp[:30]}...\")\n",
    "\n",
    "    # 4. Evaluation\n",
    "    print(\"4. Evaluating...\")\n",
    "    df_results = evaluator.run_evaluation(\n",
    "        questions=questions,\n",
    "        answers=answers,\n",
    "        contexts=contexts,\n",
    "        ground_truths=ground_truths\n",
    "    )\n",
    "    \n",
    "    # Add method column\n",
    "    df_results['method'] = method_name\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список методов для тестирования\n",
    "methods = [\n",
    "    (\"Recursive\", chunk_recursive),\n",
    "    (\"Token\", chunk_token),\n",
    "    (\"Markdown\", chunk_markdown),\n",
    "    (\"Sentence Window\", chunk_sentence_window),\n",
    "    (\"Semantic\", chunk_semantic)\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for name, func in methods:\n",
    "    df = run_pipeline_for_method(name, func, markdown_content, test_questions, ground_truths)\n",
    "    if df is not None:\n",
    "        all_results.append(df)\n",
    "\n",
    "# Объединение результатов\n",
    "if all_results:\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"\\nВсе тесты завершены.\")\n",
    "else:\n",
    "    print(\"\\nНет результатов.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36f964",
   "metadata": {},
   "source": [
    "### Анализ результатов\n",
    "Ниже приведена сводная таблица средних метрик по каждому методу чанкирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e281ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # Группировка по методу и вычисление среднего\n",
    "    numeric_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "    summary = final_df.groupby('method')[numeric_cols].mean()\n",
    "    \n",
    "    print(\"Сводная таблица (средние значения):\")\n",
    "    display(summary)\n",
    "    \n",
    "    # Сохранение полных результатов\n",
    "    final_df.to_csv(\"data/evaluation_results/chunking_comparison.csv\", index=False)\n",
    "    print(\"\\nПолные результаты сохранены в data/evaluation_results/chunking_comparison.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
