{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bc6b5a",
   "metadata": {},
   "source": [
    "# Тестирование функций и методов\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3528b",
   "metadata": {},
   "source": [
    "### Модуль Ingestion (Docling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779b9c",
   "metadata": {},
   "source": [
    "#### Загрузка и конвертация документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-23 14:02:37,881 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,882 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,893 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,895 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,255 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,256 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,279 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,280 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate file: data\\processed\\sample.md\n",
      "Result_doc: 1. Научные открытия и технологические прорывы\n",
      "\n",
      "В 2023 году учёные из ЦЕРНа подтвердили существование...\n"
     ]
    }
   ],
   "source": [
    "import src.document_loader as document_loader\n",
    "import config.config as cfg\n",
    "\n",
    "def test_load_document():\n",
    "    file_path = str(cfg.RAW_DATA_DIR / \"sample.pdf\")\n",
    "    result = document_loader.load_document(file_path)\n",
    "    print(f\"Result_doc: {result[0:100]}...\")  # Print first 100 characters for brevity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_load_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f7c69",
   "metadata": {},
   "source": [
    "#### Чанкирование по кол-ву симовлов с перекрытием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d48ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл 'sample.md' успешно разбит.\n",
      "Количество чанков: 12\n",
      "Пример первого чанка:\n",
      "'1. Научные открытия и технологические прорывы\\n\\nВ 2023 году учёные из ЦЕРНа подтвердили существование'...\n"
     ]
    }
   ],
   "source": [
    "from src.chunker import chunk_text\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "file_path = PROCESSED_DATA_DIR / \"sample.md\"\n",
    "markdown_content = file_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "CHUNK_SIZE = 500 \n",
    "OVERLAP = 100\n",
    "file_name = file_path.name # Сохраняем имя для Qdrant\n",
    "\n",
    "chunks = chunk_text(markdown_content, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "\n",
    "print(f\"Файл '{file_name}' успешно разбит.\")\n",
    "print(f\"Количество чанков: {len(chunks)}\")\n",
    "print(f\"Пример первого чанка:\\n{repr(chunks[0][:100])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ec59d",
   "metadata": {},
   "source": [
    "#### Сохранение векторизованных данных в Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb057789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерация векторов для 12 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_collection'\n"
     ]
    }
   ],
   "source": [
    "from src.embedder import vectorize_and_upload\n",
    "\n",
    "q_client, embed_model = vectorize_and_upload(chunks, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384e948",
   "metadata": {},
   "source": [
    "### Модули Retrieval и Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424063d",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756d415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 3 кандидатов через векторный поиск.\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval import retrieve\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "candidates = retrieve(user_query, q_client, embed_model, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db76377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-3 кандидата по запросу:\n",
      "1. {'text': 'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', 'score': 0.7725014352780308, 'metadata': {'source': 'sample.md', 'chunk_id': 5}}\n",
      "\n",
      "2. {'text': 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн', 'score': 0.5040614310777544, 'metadata': {'source': 'sample.md', 'chunk_id': 6}}\n",
      "\n",
      "3. {'text': ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'score': 0.3601881203512009, 'metadata': {'source': 'sample.md', 'chunk_id': 8}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Топ-3 кандидата по запросу:\")\n",
    "for i, candidate in enumerate(candidates, start=1):\n",
    "    print(f\"{i}. {candidate}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1950a",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12638658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "\n",
      "--- Релевантный чанк №1 (Score: 8.6397) ---\n",
      "аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут р...\n",
      "\n",
      "--- Релевантный чанк №2 (Score: 8.4987) ---\n",
      " содержащих стереотипы, она может их воспроизводить.\n",
      "\n",
      "В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и ра...\n",
      "\n",
      "--- Релевантный чанк №3 (Score: 7.9926) ---\n",
      "ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от...\n"
     ]
    }
   ],
   "source": [
    "from src.reranker import rerank\n",
    "\n",
    "final_context = rerank(user_query, candidates, top_n=3)\n",
    "\n",
    "# Вывод результата\n",
    "for i, res in enumerate(final_context):\n",
    "    print(f\"\\n--- Релевантный чанк №{i+1} (Score: {res['rerank_score']:.4f}) ---\")\n",
    "    print(res[\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20977c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_context[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f76b69",
   "metadata": {},
   "source": [
    "### Модуль Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfdfcb",
   "metadata": {},
   "source": [
    "#### Генератор с использованием локальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02169b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сгенерированный ответ ---\n",
      "К 2050 году потепление может достичь **2,7°C**, если выбросы парниковых газов не будут радикально сокращены.\n"
     ]
    }
   ],
   "source": [
    "from src.generator import Generator\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "response = generator.generate(user_query, final_context[0][\"text\"])\n",
    "\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196d20",
   "metadata": {},
   "source": [
    "Получение итогового списка контекста модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d4f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context_text = []\n",
    "for i in range(len(final_context)):\n",
    "    final_context_text.append(final_context[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e9c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн']\n"
     ]
    }
   ],
   "source": [
    "print(final_context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52a094",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation (Ragas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c3c57",
   "metadata": {},
   "source": [
    "#### Полный цикл оценки (Full Pipeline Evaluation)\n",
    "Включает в себя подготовку данных, генерацию ответов и расчет метрик: Faithfulness, Answer Relevancy, Context Precision, Context Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c892886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\src\\evaluation\\ragas_eval.py:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas Evaluator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.ragas_eval import RagasEvaluator\n",
    "\n",
    "# Инициализация Ragas Evaluator\n",
    "evaluator = RagasEvaluator()\n",
    "print(\"Ragas Evaluator initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cbc6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск генерации ответов...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Скольки градусам может достичь потепление к 2050 году?\n",
      "A: К 2050 году, если выбросы парниковых газов не будут радикально сокращены, потепление может достичь *...\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Какие алгоритмы DeepMind используются в медицине?\n",
      "A: Алгоритмы DeepMind, упомянутые в контексте, применяются в медицине для диагностики диабетической рет...\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Каков прогноз численности населения Земли на 2086 год?\n",
      "A: Прогноз численности населения Земли на 2086 год, согласно контексту, составляет **10,4 миллиарда чел...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка тестовых данных (Questions & Ground Truths)\n",
    "test_questions = [\n",
    "    \"Скольки градусам может достичь потепление к 2050 году?\",\n",
    "    \"Какие алгоритмы DeepMind используются в медицине?\",\n",
    "    \"Каков прогноз численности населения Земли на 2086 год?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Потепление может достичь 2,7°C.\",\n",
    "    \"Алгоритмы DeepMind помогают диагностировать диабетическую ретинопатию и рак молочной железы.\",\n",
    "    \"По прогнозам ООН, пик будет достигнут около 2086 года на уровне 10,4 миллиарда человек.\"\n",
    "]\n",
    "\n",
    "# 2. Запуск пайплайна (Inference)\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "print(\"Запуск генерации ответов...\")\n",
    "for q in test_questions:\n",
    "    # Retrieve\n",
    "    candidates = retrieve(q, q_client, embed_model, top_k=5)\n",
    "    final_c = rerank(q, candidates, top_n=3)\n",
    "    \n",
    "    # Extract context texts\n",
    "    c_texts = [c[\"text\"] for c in final_c]\n",
    "    combined_c = \"\\n\\n\".join(c_texts)\n",
    "    \n",
    "    # Generate\n",
    "    resp = generator.generate(q, combined_c)\n",
    "    \n",
    "    answers.append(resp)\n",
    "    contexts.append(c_texts)\n",
    "    print(f\"Q: {q}\\nA: {resp[:100]}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74be77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск оценки Ragas через модуль src.evaluation...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [00:41<07:31, 41.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:08<03:06, 26.63s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [04:23<01:50, 36.67s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [05:17<00:00, 26.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты оценки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Скольки градусам может достичь потепление к 20...</td>\n",
       "      <td>[аду МГЭИК (Межправительственной группы экспер...</td>\n",
       "      <td>К 2050 году, если выбросы парниковых газов не ...</td>\n",
       "      <td>Потепление может достичь 2,7°C.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Какие алгоритмы DeepMind используются в медицине?</td>\n",
       "      <td>[ содержащих стереотипы, она может их воспроиз...</td>\n",
       "      <td>Алгоритмы DeepMind, упомянутые в контексте, пр...</td>\n",
       "      <td>Алгоритмы DeepMind помогают диагностировать ди...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Каков прогноз численности населения Земли на 2...</td>\n",
       "      <td>[аду МГЭИК (Межправительственной группы экспер...</td>\n",
       "      <td>Прогноз численности населения Земли на 2086 го...</td>\n",
       "      <td>По прогнозам ООН, пик будет достигнут около 20...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999443</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Скольки градусам может достичь потепление к 20...   \n",
       "1  Какие алгоритмы DeepMind используются в медицине?   \n",
       "2  Каков прогноз численности населения Земли на 2...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [аду МГЭИК (Межправительственной группы экспер...   \n",
       "1  [ содержащих стереотипы, она может их воспроиз...   \n",
       "2  [аду МГЭИК (Межправительственной группы экспер...   \n",
       "\n",
       "                                            response  \\\n",
       "0  К 2050 году, если выбросы парниковых газов не ...   \n",
       "1  Алгоритмы DeepMind, упомянутые в контексте, пр...   \n",
       "2  Прогноз численности населения Земли на 2086 го...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0                    Потепление может достичь 2,7°C.      1.000000   \n",
       "1  Алгоритмы DeepMind помогают диагностировать ди...      1.000000   \n",
       "2  По прогнозам ООН, пик будет достигнут около 20...      0.666667   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.862986           1.000000             1.0  \n",
       "1          0.702630           1.000000             1.0  \n",
       "2          0.999443           0.333333             1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data/evaluation_results/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. Запуск оценки\n",
    "print(\"Запуск оценки Ragas через модуль src.evaluation...\")\n",
    "df_results = evaluator.run_evaluation(\n",
    "    questions=test_questions,\n",
    "    answers=answers,\n",
    "    contexts=contexts,\n",
    "    ground_truths=ground_truths\n",
    ")\n",
    "\n",
    "print(\"\\nРезультаты оценки:\")\n",
    "display(df_results)\n",
    "\n",
    "# Сохранение результатов\n",
    "evaluator.save_results(df_results, \"data/evaluation_results/evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71380f",
   "metadata": {},
   "source": [
    "## Сравнительное тестирование методов чанкирования\n",
    "---\n",
    "В этом разделе мы запускаем полный цикл RAG (Ingestion -> Chunking -> Embedding -> Retrieval -> Reranking -> Generation -> Evaluation) для каждого из 5 реализованных методов чанкирования, чтобы сравнить их эффективность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a33249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст загружен, длина: 4605 символов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\src\\evaluation\\ragas_eval.py:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Компоненты инициализированы.\n"
     ]
    }
   ],
   "source": [
    "# Импорты необходимых модулей\n",
    "import pandas as pd\n",
    "from src.chunkers.recursive_chunker import chunk_recursive\n",
    "from src.chunkers.token_chunker import chunk_token\n",
    "from src.chunkers.markdown_chunker import chunk_markdown\n",
    "from src.chunkers.sentence_window_chunker import chunk_sentence_window\n",
    "from src.chunkers.semantic_chunker import chunk_semantic\n",
    "\n",
    "from src.embedder import vectorize_and_upload\n",
    "from src.retrieval import retrieve\n",
    "from src.reranker import rerank\n",
    "from src.generator import Generator\n",
    "from src.evaluation.ragas_eval import RagasEvaluator\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "# Загрузка текста\n",
    "file_path = PROCESSED_DATA_DIR / \"sample.md\"\n",
    "markdown_content = file_path.read_text(encoding=\"utf-8\")\n",
    "print(f\"Текст загружен, длина: {len(markdown_content)} символов\")\n",
    "\n",
    "# Инициализация общих компонентов\n",
    "generator = Generator()\n",
    "evaluator = RagasEvaluator()\n",
    "print(\"Компоненты инициализированы.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37015c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые данные (Вопросы и эталонные ответы)\n",
    "test_questions = [\n",
    "    \"Скольки градусам может достичь потепление к 2050 году?\",\n",
    "    \"Какие алгоритмы DeepMind используются в медицине?\",\n",
    "    \"Каков прогноз численности населения Земли на 2086 год?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Потепление может достичь 2,7°C.\",\n",
    "    \"Алгоритмы DeepMind помогают диагностировать диабетическую ретинопатию и рак молочной железы.\",\n",
    "    \"По прогнозам ООН, пик будет достигнут около 2086 года на уровне 10,4 миллиарда человек.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf153f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_for_method(method_name, chunk_func, text, questions, ground_truths):\n",
    "    print(f\"\\n{'='*20} Testing Method: {method_name} {'='*20}\")\n",
    "    \n",
    "    # 1. Chunking\n",
    "    print(\"1. Chunking...\")\n",
    "    try:\n",
    "        # Некоторые методы могут требовать доп. параметров, но мы используем дефолтные для теста\n",
    "        chunks = chunk_func(text)\n",
    "        print(f\"   Получено {len(chunks)} чанков.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Ошибка при чанкировании: {e}\")\n",
    "        return None\n",
    "        \n",
    "    if not chunks:\n",
    "        print(\"   Чанки не созданы.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Embedding & Upload (Unique Collection)\n",
    "    collection_name = f\"docs_{method_name.lower().replace(' ', '_')}\"\n",
    "    print(f\"2. Embedding to collection '{collection_name}'...\")\n",
    "    client, embed_model = vectorize_and_upload(chunks, \"sample.md\", collection_name=collection_name)\n",
    "\n",
    "    # 3. Retrieval, Reranking, Generation\n",
    "    print(\"3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\")\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # Retrieve\n",
    "        candidates = retrieve(q, client, embed_model, top_k=5, collection_name=collection_name)\n",
    "        # Rerank\n",
    "        final_c = rerank(q, candidates, top_n=3)\n",
    "        \n",
    "        # Extract context texts\n",
    "        c_texts = [c[\"text\"] for c in final_c]\n",
    "        combined_c = \"\\n\\n\".join(c_texts)\n",
    "        \n",
    "        # Generate\n",
    "        resp = generator.generate(q, combined_c)\n",
    "        answers.append(resp)\n",
    "        contexts.append(c_texts)\n",
    "        # print(f\"   Q: {q[:30]}... -> A: {resp[:30]}...\")\n",
    "\n",
    "    # 4. Evaluation\n",
    "    print(\"4. Evaluating...\")\n",
    "    df_results = evaluator.run_evaluation(\n",
    "        questions=questions,\n",
    "        answers=answers,\n",
    "        contexts=contexts,\n",
    "        ground_truths=ground_truths\n",
    "    )\n",
    "    \n",
    "    # Add method column\n",
    "    df_results['method'] = method_name\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d1777c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing Method: Recursive ====================\n",
      "1. Chunking...\n",
      "   Получено 6 чанков.\n",
      "2. Embedding to collection 'docs_recursive'...\n",
      "Генерация векторов для 6 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_recursive'\n",
      "3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "4. Evaluating...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [00:32<05:57, 32.47s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:22<03:55, 33.70s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [03:46<01:14, 24.74s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [04:40<00:00, 23.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing Method: Token ====================\n",
      "1. Chunking...\n",
      "   Получено 11 чанков.\n",
      "2. Embedding to collection 'docs_token'...\n",
      "Генерация векторов для 11 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_token'\n",
      "3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "4. Evaluating...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [01:08<12:34, 68.60s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:48<03:45, 32.20s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [04:31<01:28, 29.47s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [05:26<00:00, 27.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing Method: Markdown ====================\n",
      "1. Chunking...\n",
      "   Получено 4 чанков.\n",
      "2. Embedding to collection 'docs_markdown'...\n",
      "Генерация векторов для 4 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_markdown'\n",
      "3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\n",
      "Найдено 4 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 4 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 4 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "4. Evaluating...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [00:46<08:28, 46.24s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  33%|███▎      | 4/12 [01:37<02:43, 20.42s/it]Exception raised in Job[4]: InstructorRetryException(<failed_attempts>\n",
      "\n",
      "<generation number=\"1\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-948', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to judge faithfulness of each statement relative to context. Provide JSON with statements array each containing statement, reason, verdict 0/1.\\n\\nWe must parse context: It\\'s Russian text. Let\\'s translate key parts:\\n\\nContext:\\n\\n[Header 2: 4. Искусственный интеллект и этика]\\nWith development of large language models (LLM) such as GPT-4, Llama 3, Claude 3, serious ethical questions arise. For example, can AI systems have copyright? In 2023 US court denied registration of copyright for image created by Midjourney, stating that author can only be a person. Also bias issues: if model trained on data containing stereotypes, it may reproduce them.\\n\\nAt same time AI actively applied in medicine: DeepMind algorithms (Google subsidiary) help diagnose diabetic retinopathy and breast cancer with accuracy comparable to expert humans.\\n\\n[Header 2: 6. Разное: интересные факты и парадоксы]\\n- Fermi paradox...\\n- Water...\\n- In Ancient Egypt cats were sacred; killing cat punished by death.\\n- PageRank algorithm was developed by Larry Page and Sergey Brin in 1996 at Stanford.\\n\\n[Header 2: 5. География и демография]\\nPopulation of Earth reached 8 billion in Nov 2022. UN forecast peak 2086 at 10.4 bn. Aging population problem in Japan, South Korea, EU, while sub-Saharan Africa continues rapid growth of youth. Most populous country is not China but India, which in 2023 surpassed China by population (~1.428 bn vs 1.425 bn). Highest density in Bangladesh (1265 per km2) and Monaco (>19,000 per km2).\\n\\nNow statements:\\n\\n1. \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\"\\n2. \"Алгоритмы DeepMind применяются в медицине.\"\\n3. \"Алгоритмы DeepMind применяются для диагностики заболеваний.\"\\n4. \"Алгоритмы DeepMind помогают в диагностике диабетической ретинопатии.\"\\n5. \"Алгоритмы DeepMind помогают в диагностике рака молочной железы.\"\\n6. \"Алгоритмы DeepMind помогают с точностью, сопоставимой с экспертизой врачей-людей.\"\\n7. \"Эффективность искусственного интеллекта в медицинской диагностике демонстрируется.\"\\n8. \"Эффективность искусственного интеллекта в медицинской диагностике подтверждена, как указано в контексте.\"\\n\\nWe need to judge each statement\\'s faithfulness: can be directly inferred from context.\\n\\nLet\\'s analyze each:\\n\\nContext says: \"В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\"\\n\\nThus:\\n\\n- Statement 1: \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\" The context says \"DeepMind (подразделение Google)\". It says DeepMind is a subsidiary of Google. It doesn\\'t explicitly say \"разработаны в рамках подразделения Google\" but implies that DeepMind is a Google subsidiary. The statement says \"algorithms DeepMind were developed within the Google subsidiary.\" The context says \"DeepMind (подразделение Google)\". That indicates DeepMind is a Google subsidiary. It doesn\\'t explicitly say algorithms were developed within Google, but it\\'s implied that DeepMind is part of Google. The statement is basically \"DeepMind algorithms were developed within the Google subsidiary.\" The context says \"DeepMind (подразделение Google)\". That indicates DeepMind is a Google subsidiary. So yes, we can infer that DeepMind is a Google subsidiary. The statement says \"algorithms DeepMind were developed within the Google subsidiary.\" That is basically the same. So verdict 1.\\n\\n- Statement 2: \"Алгоритмы DeepMind применяются в медицине.\" The context says \"ИИ активно применяется в медицине: алгоритмы DeepMind ... помогают диагностировать ...\". So yes, they are applied in medicine. Verdict 1.\\n\\n- Statement 3: \"Алгоритмы DeepMind применяются для диагностики заболеваний.\" The context says they help diagnose diabetic retinopathy and breast cancer. So yes, they are used for diagnosing diseases. Verdict 1.\\n\\n- Statement 4: \"Алгоритмы DeepMind помогают в диагностике диабетической ретинопатии.\" The context explicitly says \"помогают диагностировать диабетическую ретинопатию\". So yes. Verdict 1.\\n\\n- Statement 5: \"Алгоритмы DeepMind помогают в диагностике рака молочной железы.\" The context says \"помогают диагностировать ... рак молочной железы\". So yes. Verdict 1.\\n\\n- Statement 6: \"Алгоритмы DeepMind помогают с точностью, сопоставимой с экспер'))], created=1769173294, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=5607, total_tokens=8679, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"2\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-598', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to judge each statement\\'s faithfulness to the context. The context is in Russian. Let\\'s parse.\\n\\nContext:\\n\\n[Header 2: 4. Искусственный интеллект и этика]\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возникают серьёзные этические вопросы. Например, могут ли ИИ-системы обладать авторскими правами? В 2023 году суд США отказал в регистрации авторства на изображение, созданное Midjourney, заявив, что автором может быть только человек. Также актуальны проблемы предвзятости: если модель обучена на данных, содержащих стереотипы, она может их воспроизводить.  \\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n[Header 2: 6. Разное: интересные факты и парадоксы]\\n- Парадокс Ферми: если во Вселенной так много звёзд и планет, почему мы до сих пор не обнаружили внеземную цивилизацию?\\n- Вода - одно из немногих веществ, плотность которого в жидком состоянии выше, чем в твёрдом (лёд плавает).\\n- В Древнем Египте кошки считались священными животными; убийство кошки каралось смертной казнью.\\n- Алгоритм PageRank, лежащий в основе раннего Google, был разработан Ларри Пейджем и Сергеем Брином в 1996 году в Стэнфордском университете.\\n\\n[Header 2: 5. География и демография]\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблемой в Японии, Южной Корее и странах ЕС, тогда как в Африке к югу от Сахары продолжается быстрый рост численности молодёжи.  \\n\\nСамая густонаселённая страна - не Китай, а Индия, которая, по данным 2023 года, обогнала Китай по численности населения (около 1,428 млрд против 1,425 млрд). Однако плотность населения выше в таких странах, как Бангладеш (1265 чел./км²) и Монако (более 19 000 чел./км²).\\n\\nStatements:\\n\\n1. \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\"\\n2. \"Алгоритмы DeepMind применяются в медицине.\"\\n3. \"Алгоритмы DeepMind применяются для диагностики заболеваний.\"\\n4. \"Алгоритмы DeepMind помогают в диагностике диабетической ретинопатии.\"\\n5. \"Алгоритмы DeepMind помогают в диагностике рака молочной железы.\"\\n6. \"Алгоритмы DeepMind помогают с точностью, сопоставимой с экспертизой врачей-людей.\"\\n7. \"Эффективность искусственного интеллекта в медицинской диагностике демонстрируется.\"\\n8. \"Эффективность искусственного интеллекта в медицинской диагностике подтверждена, как указано в контексте.\"\\n\\nWe need to judge each statement: can be directly inferred from context? If yes, verdict 1; else 0.\\n\\nLet\\'s analyze each.\\n\\nContext says: \"В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\"\\n\\nThus:\\n\\n- Statement 1: \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\" The context says \"алгоритмы DeepMind (подразделение Google)\". That indicates DeepMind is a subdivision of Google. It doesn\\'t explicitly say \"разработаны в рамках подразделения Google\", but it says they are part of Google. The statement says \"разработаны в рамках подразделения Google.\" The context says \"алгоритмы DeepMind (подразделение Google)\". That implies they are part of Google. It doesn\\'t explicitly say \"разработаны в рамках подразделения Google\", but it\\'s implied that DeepMind is a subdivision of Google. The statement is basically \"DeepMind algorithms were developed within the Google division.\" The context says \"DeepMind (подразделение Google)\". That indicates DeepMind is a subdivision of Google. So yes, we can infer that the algorithms are developed within that'))], created=1769173322, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=5607, total_tokens=8679, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"3\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-139', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to judge each statement\\'s faithfulness to the context. The context is in Russian. Let\\'s parse.\\n\\nContext:\\n\\n[Header 2: 4. Искусственный интеллект и этика]\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возникают серьёзные этические вопросы. Например, могут ли ИИ-системы обладать авторскими правами? В 2023 году суд США отказал в регистрации авторства на изображение, созданное Midjourney, заявив, что автором может быть только человек. Также актуальны проблемы предвзятости: если модель обучена на данных, содержащих стереотипы, она может их воспроизводить.  \\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n[Header 2: 6. Разное: интересные факты и парадоксы]\\n- Парадокс Ферми: если во Вселенной так много звёзд и планет, почему мы до сих пор не обнаружили внеземную цивилизацию?\\n- Вода - одно из немногих веществ, плотность которого в жидком состоянии выше, чем в твёрдом (лёд плавает).\\n- В Древнем Египте кошки считались священными животными; убийство кошки каралось смертной казнью.\\n- Алгоритм PageRank, лежащий в основе раннего Google, был разработан Ларри Пейджем и Сергеем Брином в 1996 году в Стэнфордском университете.\\n\\n[Header 2: 5. География и демография]\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблемой в Японии, Южной Корее и странах ЕС, тогда как в Африке к югу от Сахары продолжается быстрый рост численности молодёжи.  \\n\\nСамая густонаселённая страна - не Китай, а Индия, которая, по данным 2023 года, обогнала Китай по численности населения (около 1,428 млрд против 1,425 млрд). Однако плотность населения выше в таких странах, как Бангладеш (1265 чел./км²) и Монако (более 19 000 чел./км²).\\n\\nStatements:\\n\\n1. \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\"\\n2. \"Алгоритмы DeepMind применяются в медицине.\"\\n3. \"Алгоритмы DeepMind применяются для диагностики заболеваний.\"\\n4. \"Алгоритмы DeepMind помогают в диагностике диабетической ретинопатии.\"\\n5. \"Алгоритмы DeepMind помогают в диагностике рака молочной железы.\"\\n6. \"Алгоритмы DeepMind помогают с точностью, сопоставимой с экспертизой врачей-людей.\"\\n7. \"Эффективность искусственного интеллекта в медицинской диагностике демонстрируется.\"\\n8. \"Эффективность искусственного интеллекта в медицинской диагностике подтверждена, как указано в контексте.\"\\n\\nWe need to judge each statement: can be directly inferred from context? If yes, verdict 1; else 0.\\n\\nLet\\'s analyze each.\\n\\nContext says: \"В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\"\\n\\nThus:\\n\\n- Statement 1: \"Алгоритмы DeepMind разработаны в рамках подразделения Google.\" The context says \"алгоритмы DeepMind (подразделение Google)\". That indicates DeepMind is a subdivision of Google. It doesn\\'t explicitly say \"разработаны в рамках подразделения Google\", but it says they are part of Google. The statement says \"разработаны в рамках подразделения Google.\" The context says \"алгоритмы DeepMind (подразделение Google)\". That implies they are part of Google. It doesn\\'t explicitly say \"разработаны в рамках подразделения Google\", but it\\'s implied that DeepMind is a subdivision of Google. The statement is basically \"DeepMind algorithms were developed within the Google division.\" The context says \"DeepMind (подразделение Google)\". That indicates DeepMind is a subdivision of Google. So yes, we can infer that the algorithms are developed within that'))], created=1769173350, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=5607, total_tokens=8679, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "</failed_attempts>\n",
      "\n",
      "<last_exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</last_exception>)\n",
      "Evaluating:  42%|████▏     | 5/12 [03:34<06:26, 55.25s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  67%|██████▋   | 8/12 [04:24<01:54, 28.70s/it]Exception raised in Job[8]: InstructorRetryException(<failed_attempts>\n",
      "\n",
      "<generation number=\"1\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-788', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to parse the answer into statements, no pronouns. Each statement should be fully understandable, no pronouns. Use double quotes, properly escaped with backslash? The instruction: \"Do not use single quotes in your response but double quotes, properly escaped with a backslash.\" That means we need to escape double quotes inside strings? Actually JSON uses double quotes for keys and string values. Inside string, if we need to include double quotes, we escape them with backslash. But we likely don\\'t need internal quotes. So just normal JSON.\\n\\nWe need to produce \"statements\" array. Let\\'s analyze answer sentences.\\n\\nAnswer text:\\n\\n\"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к **2086 году** может составить **около 10,4 миллиарда человек**. Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах. \\\\n\\\\nОднако точные прогнозы зависят от множества факторов:  \\\\n- **Рождаемость** и смертность (например, улучшение медицинского обслуживания может снизить смертность).  \\\\n- **Экономические и социальные условия** (например, миграция, войны, кризисы).  \\\\n- **Экологические и климатические изменения**.  \\\\n\\\\nДругие сценарии могут предсказывать:  \\\\n- **Низкий сценарий** (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.  \\\\n- **Высокий сценарий** (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.  \\\\n\\\\nТаким образом, **10,4 миллиарда** — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to break into statements. Let\\'s list sentences:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Однако точные прогнозы зависят от множества факторов:\"\\n4. \"- Рождаемость и смертность (например, улучшение медицинского обслуживания может снизить смертность).\"\\n5. \"- Экономические и социальные условия (например, миграция, войны, кризисы).\"\\n6. \"- Экологические и климатические изменения.\"\\n7. \"Другие сценарии могут предсказывать:\"\\n8. \"- Низкий сценарий (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.\"\\n9. \"- Высокий сценарий (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.\"\\n10. \"Таким образом, 10,4 миллиарда — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to transform each into statements with no pronouns. Replace \"Этот\" with \"Этот сценарий\" but pronoun \"этот\" is a demonstrative pronoun. We need to avoid pronouns. So we need to rephrase: \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\" That uses no pronouns. Similarly \"Однако точные прогнозы зависят от множества факторов:\" we can rephrase: \"Точные прогнозы зависят от множества факторов:\" but \"точные\" is adjective, fine. \"Однако\" is conjunction, okay. But \"точные прогнозы\" no pronoun. Good.\\n\\nFor bullet points, we need separate statements: \"Факторы, влияющие на точные прогнозы, включают рождаемость и смертность, где улучшение медицинского обслуживания может снизить смертность.\" etc.\\n\\nLet\\'s craft statements:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Точные прогнозы зависят от множества факторов.\"\\n4. \"Факторы'))], created=1769173428, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=2505, total_tokens=5577, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"2\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-555', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to parse the answer into statements, no pronouns. Each statement should be fully understandable, no pronouns. Use double quotes, properly escaped with backslash? The instruction: \"Do not use single quotes in your response but double quotes, properly escaped with a backslash.\" That means we need to escape double quotes inside strings? Actually JSON uses double quotes for keys and string values. Inside string, if we need to include double quotes, we escape them with backslash. But we likely don\\'t need internal quotes. So just normal JSON.\\n\\nWe need to produce \"statements\" array. Let\\'s analyze answer sentences.\\n\\nAnswer text:\\n\\n\"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к **2086 году** может составить **около 10,4 миллиарда человек**. Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах. \\\\n\\\\nОднако точные прогнозы зависят от множества факторов:  \\\\n- **Рождаемость** и смертность (например, улучшение медицинского обслуживания может снизить смертность).  \\\\n- **Экономические и социальные условия** (например, миграция, войны, кризисы).  \\\\n- **Экологические и климатические изменения**.  \\\\n\\\\nДругие сценарии могут предсказывать:  \\\\n- **Низкий сценарий** (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.  \\\\n- **Высокий сценарий** (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.  \\\\n\\\\nТаким образом, **10,4 миллиарда** — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to break into statements. Let\\'s list sentences:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Однако точные прогнозы зависят от множества факторов:\"\\n4. \"- Рождаемость и смертность (например, улучшение медицинского обслуживания может снизить смертность).\"\\n5. \"- Экономические и социальные условия (например, миграция, войны, кризисы).\"\\n6. \"- Экологические и климатические изменения.\"\\n7. \"Другие сценарии могут предсказывать:\"\\n8. \"- Низкий сценарий (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.\"\\n9. \"- Высокий сценарий (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.\"\\n10. \"Таким образом, 10,4 миллиарда — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to transform each into statements with no pronouns. Replace \"Этот\" with \"Этот сценарий\" but pronoun \"этот\" is a demonstrative pronoun. We need to avoid pronouns. So we need to rephrase: \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\" That uses no pronouns. Similarly \"Однако точные прогнозы зависят от множества факторов:\" we can rephrase: \"Точные прогнозы зависят от множества факторов:\" but \"точные\" is adjective, fine. \"Однако\" is conjunction, okay. But \"точные прогнозы\" no pronoun. Good.\\n\\nFor bullet points, we need separate statements: \"Факторы, влияющие на точные прогнозы, включают рождаемость и смертность, где улучшение медицинского обслуживания может снизить смертность.\" etc.\\n\\nLet\\'s craft statements:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Точные прогнозы зависят от множества факторов.\"\\n4. \"Факторы'))], created=1769173456, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=2505, total_tokens=5577, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"3\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-121', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to parse the answer into statements, no pronouns. Each statement should be fully understandable, no pronouns. Use double quotes, properly escaped with backslash? The instruction: \"Do not use single quotes in your response but double quotes, properly escaped with a backslash.\" That means we need to escape double quotes inside strings? Actually JSON uses double quotes for keys and string values. Inside string, if we need to include double quotes, we escape them with backslash. But we likely don\\'t need internal quotes. So just normal JSON.\\n\\nWe need to produce \"statements\" array. Let\\'s analyze answer sentences.\\n\\nAnswer text:\\n\\n\"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к **2086 году** может составить **около 10,4 миллиарда человек**. Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах. \\\\n\\\\nОднако точные прогнозы зависят от множества факторов:  \\\\n- **Рождаемость** и смертность (например, улучшение медицинского обслуживания может снизить смертность).  \\\\n- **Экономические и социальные условия** (например, миграция, войны, кризисы).  \\\\n- **Экологические и климатические изменения**.  \\\\n\\\\nДругие сценарии могут предсказывать:  \\\\n- **Низкий сценарий** (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.  \\\\n- **Высокий сценарий** (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.  \\\\n\\\\nТаким образом, **10,4 миллиарда** — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to break into statements. Let\\'s list sentences:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Этот сценарий предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Однако точные прогнозы зависят от множества факторов:\"\\n4. \"- Рождаемость и смертность (например, улучшение медицинского обслуживания может снизить смертность).\"\\n5. \"- Экономические и социальные условия (например, миграция, войны, кризисы).\"\\n6. \"- Экологические и климатические изменения.\"\\n7. \"Другие сценарии могут предсказывать:\"\\n8. \"- Низкий сценарий (например, 9,5–10 миллиардов) при более быстром снижении рождаемости.\"\\n9. \"- Высокий сценарий (11–12 миллиардов) при сохранении высоких темпов роста в развивающихся странах.\"\\n10. \"Таким образом, 10,4 миллиарда — это наиболее вероятная оценка по среднему сценарию ООН, но реальные цифры могут отличаться.\"\\n\\nWe need to transform each into statements with no pronouns. Replace \"Этот\" with \"Этот сценарий\" but pronoun \"этот\" is a demonstrative pronoun. We need to avoid pronouns. So we need to rephrase: \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\" That uses no pronouns. Similarly \"Однако точные прогнозы зависят от множества факторов:\" we can rephrase: \"Точные прогнозы зависят от множества факторов:\" but \"точные\" is adjective, fine. \"Однако\" is conjunction, okay. But \"точные прогнозы\" no pronoun. Good.\\n\\nFor bullet points, we need separate statements: \"Факторы, влияющие на точные прогнозы, включают рождаемость и смертность, где улучшение медицинского обслуживания может снизить смертность.\" etc.\\n\\nLet\\'s craft statements:\\n\\n1. \"Согласно прогнозам ООН, опубликованным в 2022 году, численность населения Земли к 2086 году может составить около 10,4 миллиарда человек.\"\\n2. \"Сценарий, основанный на прогнозах ООН, предполагает умеренные темпы роста населения, учитывая тенденции снижения рождаемости в развитых странах и умеренное увеличение численности в развивающихся регионах.\"\\n3. \"Точные прогнозы зависят от множества факторов.\"\\n4. \"Факторы'))], created=1769173483, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=2505, total_tokens=5577, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "</failed_attempts>\n",
      "\n",
      "<last_exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</last_exception>)\n",
      "Evaluating:  75%|███████▌  | 9/12 [05:47<02:17, 45.80s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [06:38<00:00, 33.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing Method: Sentence Window ====================\n",
      "1. Chunking...\n",
      "   Получено 35 чанков.\n",
      "2. Embedding to collection 'docs_sentence_window'...\n",
      "Генерация векторов для 35 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_sentence_window'\n",
      "3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "4. Evaluating...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [01:04<11:50, 64.57s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:42<03:36, 30.96s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [04:01<01:09, 23.20s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [04:47<00:00, 23.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing Method: Semantic ====================\n",
      "1. Chunking...\n",
      "   Получено 35 чанков.\n",
      "2. Embedding to collection 'docs_semantic'...\n",
      "Генерация векторов для 35 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_semantic'\n",
      "3. Running RAG Pipeline (Retrieve -> Rerank -> Generate)...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "4. Evaluating...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 1/12 [00:35<06:25, 35.09s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 5/12 [02:29<04:02, 34.63s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▌  | 9/12 [04:31<01:48, 36.32s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [05:25<00:00, 27.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Все тесты завершены.\n"
     ]
    }
   ],
   "source": [
    "# Список методов для тестирования\n",
    "methods = [\n",
    "    (\"Recursive\", chunk_recursive),\n",
    "    (\"Token\", chunk_token),\n",
    "    (\"Markdown\", chunk_markdown),\n",
    "    (\"Sentence Window\", chunk_sentence_window),\n",
    "    (\"Semantic\", chunk_semantic)\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for name, func in methods:\n",
    "    df = run_pipeline_for_method(name, func, markdown_content, test_questions, ground_truths)\n",
    "    if df is not None:\n",
    "        all_results.append(df)\n",
    "\n",
    "# Объединение результатов\n",
    "if all_results:\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"\\nВсе тесты завершены.\")\n",
    "else:\n",
    "    print(\"\\nНет результатов.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36f964",
   "metadata": {},
   "source": [
    "### Анализ результатов\n",
    "Ниже приведена сводная таблица средних метрик по каждому методу чанкирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e281ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сводная таблица (средние значения):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Markdown</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.889060</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.545071</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semantic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842005</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence Window</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845251</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847919</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 faithfulness  answer_relevancy  context_precision  \\\n",
       "method                                                               \n",
       "Markdown             0.750000          0.889060           0.666667   \n",
       "Recursive            0.666667          0.545071           0.833333   \n",
       "Semantic             1.000000          0.842005           0.666667   \n",
       "Sentence Window      1.000000          0.845251           0.833333   \n",
       "Token                1.000000          0.847919           0.611111   \n",
       "\n",
       "                 context_recall  \n",
       "method                           \n",
       "Markdown               0.666667  \n",
       "Recursive              1.000000  \n",
       "Semantic               0.666667  \n",
       "Sentence Window        1.000000  \n",
       "Token                  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Полные результаты сохранены в data/evaluation_results/chunking_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "if all_results:\n",
    "    # Группировка по методу и вычисление среднего\n",
    "    numeric_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "    summary = final_df.groupby('method')[numeric_cols].mean()\n",
    "    \n",
    "    print(\"Сводная таблица (средние значения):\")\n",
    "    display(summary)\n",
    "    \n",
    "    # Сохранение полных результатов\n",
    "    final_df.to_csv(\"data/evaluation_results/chunking_comparison.csv\", index=False)\n",
    "    print(\"\\nПолные результаты сохранены в data/evaluation_results/chunking_comparison.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
