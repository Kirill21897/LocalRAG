{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bc6b5a",
   "metadata": {},
   "source": [
    "# Тестирование функций и методов\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3528b",
   "metadata": {},
   "source": [
    "### Модуль Ingestion (Docling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779b9c",
   "metadata": {},
   "source": [
    "#### Загрузка и конвертация документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-23 14:02:37,881 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,882 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,893 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:37,895 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,205 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,206 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,255 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,256 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,279 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-23 14:02:38,280 [RapidOCR] main.py:50: Using D:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate file: data\\processed\\sample.md\n",
      "Result_doc: 1. Научные открытия и технологические прорывы\n",
      "\n",
      "В 2023 году учёные из ЦЕРНа подтвердили существование...\n"
     ]
    }
   ],
   "source": [
    "import src.document_loader as document_loader\n",
    "import config.config as cfg\n",
    "\n",
    "def test_load_document():\n",
    "    file_path = str(cfg.RAW_DATA_DIR / \"sample.pdf\")\n",
    "    result = document_loader.load_document(file_path)\n",
    "    print(f\"Result_doc: {result[0:100]}...\")  # Print first 100 characters for brevity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_load_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f7c69",
   "metadata": {},
   "source": [
    "#### Чанкирование по кол-ву симовлов с перекрытием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d48ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл 'sample.md' успешно разбит.\n",
      "Количество чанков: 12\n",
      "Пример первого чанка:\n",
      "'1. Научные открытия и технологические прорывы\\n\\nВ 2023 году учёные из ЦЕРНа подтвердили существование'...\n"
     ]
    }
   ],
   "source": [
    "from src.chunker import chunk_text\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "file_path = PROCESSED_DATA_DIR / \"sample.md\"\n",
    "markdown_content = file_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "CHUNK_SIZE = 500 \n",
    "OVERLAP = 100\n",
    "file_name = file_path.name # Сохраняем имя для Qdrant\n",
    "\n",
    "chunks = chunk_text(markdown_content, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "\n",
    "print(f\"Файл '{file_name}' успешно разбит.\")\n",
    "print(f\"Количество чанков: {len(chunks)}\")\n",
    "print(f\"Пример первого чанка:\\n{repr(chunks[0][:100])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ec59d",
   "metadata": {},
   "source": [
    "#### Сохранение векторизованных данных в Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb057789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерация векторов для 12 чанков...\n",
      "Данные успешно загружены в коллекцию 'docs_collection'\n"
     ]
    }
   ],
   "source": [
    "from src.embedder import vectorize_and_upload\n",
    "\n",
    "q_client, embed_model = vectorize_and_upload(chunks, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384e948",
   "metadata": {},
   "source": [
    "### Модули Retrieval и Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424063d",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756d415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 3 кандидатов через векторный поиск.\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval import retrieve\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "candidates = retrieve(user_query, q_client, embed_model, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db76377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-3 кандидата по запросу:\n",
      "1. {'text': 'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', 'score': 0.7725014352780308, 'metadata': {'source': 'sample.md', 'chunk_id': 5}}\n",
      "\n",
      "2. {'text': 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн', 'score': 0.5040614310777544, 'metadata': {'source': 'sample.md', 'chunk_id': 6}}\n",
      "\n",
      "3. {'text': ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'score': 0.3601881203512009, 'metadata': {'source': 'sample.md', 'chunk_id': 8}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Топ-3 кандидата по запросу:\")\n",
    "for i, candidate in enumerate(candidates, start=1):\n",
    "    print(f\"{i}. {candidate}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1950a",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12638658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "\n",
      "--- Релевантный чанк №1 (Score: 8.6397) ---\n",
      "аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут р...\n",
      "\n",
      "--- Релевантный чанк №2 (Score: 8.4987) ---\n",
      " содержащих стереотипы, она может их воспроизводить.\n",
      "\n",
      "В то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и ра...\n",
      "\n",
      "--- Релевантный чанк №3 (Score: 7.9926) ---\n",
      "ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от...\n"
     ]
    }
   ],
   "source": [
    "from src.reranker import rerank\n",
    "\n",
    "final_context = rerank(user_query, candidates, top_n=3)\n",
    "\n",
    "# Вывод результата\n",
    "for i, res in enumerate(final_context):\n",
    "    print(f\"\\n--- Релевантный чанк №{i+1} (Score: {res['rerank_score']:.4f}) ---\")\n",
    "    print(res[\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20977c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_context[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f76b69",
   "metadata": {},
   "source": [
    "### Модуль Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfdfcb",
   "metadata": {},
   "source": [
    "#### Генератор с использованием локальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02169b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сгенерированный ответ ---\n",
      "Согласно предоставленному контексту, к 2050 году потепление может достичь 2,7°C по сравнению с довоенным уровнем.\n"
     ]
    }
   ],
   "source": [
    "from src.generator import Generator\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "user_query = \"Скольки градусам может достичь потепление к 2050 году?\"\n",
    "response = generator.generate(user_query, final_context[0][\"text\"])\n",
    "\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196d20",
   "metadata": {},
   "source": [
    "Получение итогового списка контекста модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d4f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context_text = []\n",
    "for i in range(len(final_context)):\n",
    "    final_context_text.append(final_context[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e9c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аду МГЭИК (Межправительственной группы экспертов по изменению климата) за 2022 год, глобальная средняя температура уже на 1,15°C выше доиндустриального уровня. Если выбросы парниковых газов не будут радикально сокращены, к 2050 году потепление может достичь 2,7°C, что приведёт к катастрофическим последствиям: повышению уровня моря, экстремальным погодным явлениям и массовому вымиранию видов.\\n\\nОднако существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утвержд', ' содержащих стереотипы, она может их воспроизводить.\\n\\nВ то же время ИИ активно применяется в медицине: алгоритмы DeepMind (подразделение Google) помогают диагностировать диабетическую ретинопатию и рак молочной железы с точностью, сопоставимой с экспертами-людьми.\\n\\n## 5. География и демография\\n\\nНаселение Земли достигло 8 миллиардов человек в ноябре 2022 года. По прогнозам ООН, пик будет достигнут около 2086 года - на уровне 10,4 миллиарда. При этом старение населения становится серьёзной проблем', 'ко существуют и контраргументы: некоторые исследователи, такие как климатолог Бьорн Ломборг, утверждают, что адаптация к изменениям климата может быть более эффективной стратегией, чем полный отказ от ископаемого топлива. Тем не менее, большинство научного сообщества поддерживает Парижское соглашение 2015 года, целью которого является удержание роста температуры ниже 2°C.\\n\\n## 4. Искусственный интеллект и этика\\n\\nС развитием больших языковых моделей (LLM), таких как GPT-4, Llama 3 и Claude 3, возн']\n"
     ]
    }
   ],
   "source": [
    "print(final_context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52a094",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation (Ragas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c3c57",
   "metadata": {},
   "source": [
    "#### Полный цикл оценки (Full Pipeline Evaluation)\n",
    "Включает в себя подготовку данных, генерацию ответов и расчет метрик: Faithfulness, Answer Relevancy, Context Precision, Context Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c892886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LocalRAG\\src\\evaluation\\ragas_eval.py:32: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas Evaluator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.ragas_eval import RagasEvaluator\n",
    "\n",
    "# Инициализация Ragas Evaluator\n",
    "evaluator = RagasEvaluator()\n",
    "print(\"Ragas Evaluator initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70cbc6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск генерации ответов...\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Скольки градусам может достичь потепление к 2050 году?\n",
      "A: Согласно предоставленному контексту, к 2050 году потепление может достичь 2,7°C....\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Какие алгоритмы DeepMind используются в медицине?\n",
      "A: Контекст упоминает об использовании алгоритмов DeepMind для диагностики и лечения рака с помощью ней...\n",
      "\n",
      "Найдено 5 кандидатов через векторный поиск.\n",
      "Переранжирование завершено. Выбрано топ-3 наиболее релевантных.\n",
      "Q: Каков прогноз численности населения Земли на 2086 год?\n",
      "A: По данным из контекста, до 2086 года预计到2086年，世界人口可能会增长到104亿左右。请注意，这里的“10,4”指的是十亿中的个位数为4，即104亿。这只是一个模...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка тестовых данных (Questions & Ground Truths)\n",
    "test_questions = [\n",
    "    \"Скольки градусам может достичь потепление к 2050 году?\",\n",
    "    \"Какие алгоритмы DeepMind используются в медицине?\",\n",
    "    \"Каков прогноз численности населения Земли на 2086 год?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Потепление может достичь 2,7°C.\",\n",
    "    \"Алгоритмы DeepMind помогают диагностировать диабетическую ретинопатию и рак молочной железы.\",\n",
    "    \"По прогнозам ООН, пик будет достигнут около 2086 года на уровне 10,4 миллиарда человек.\"\n",
    "]\n",
    "\n",
    "# 2. Запуск пайплайна (Inference)\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "print(\"Запуск генерации ответов...\")\n",
    "for q in test_questions:\n",
    "    # Retrieve\n",
    "    candidates = retrieve(q, q_client, embed_model, top_k=5)\n",
    "    final_c = rerank(q, candidates, top_n=3)\n",
    "    \n",
    "    # Extract context texts\n",
    "    c_texts = [c[\"text\"] for c in final_c]\n",
    "    combined_c = \"\\n\\n\".join(c_texts)\n",
    "    \n",
    "    # Generate\n",
    "    resp = generator.generate(q, combined_c)\n",
    "    \n",
    "    answers.append(resp)\n",
    "    contexts.append(c_texts)\n",
    "    print(f\"Q: {q}\\nA: {resp[:100]}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74be77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск оценки Ragas через модуль src.evaluation...\n",
      "Starting Ragas evaluation pipeline...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RunConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4. Запуск оценки\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mЗапуск оценки Ragas через модуль src.evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswers\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mground_truths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mground_truths\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mРезультаты оценки:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m display(df_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\LocalRAG\\src\\evaluation\\ragas_eval.py:78\u001b[39m, in \u001b[36mRagasEvaluator.run_evaluation\u001b[39m\u001b[34m(self, questions, answers, contexts, ground_truths)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Ragas evaluation pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Configure run to use single thread to avoid local LLM overload\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# and set a high timeout for the run configuration itself if applicable\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m my_run_config = \u001b[43mRunConfig\u001b[49m(timeout=\u001b[32m1200\u001b[39m, max_workers=\u001b[32m1\u001b[39m)\n\u001b[32m     80\u001b[39m results = evaluate(\n\u001b[32m     81\u001b[39m     dataset=dataset,\n\u001b[32m     82\u001b[39m     metrics=\u001b[38;5;28mself\u001b[39m.metrics,\n\u001b[32m     83\u001b[39m     run_config=my_run_config,\n\u001b[32m     84\u001b[39m     raise_exceptions=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m     85\u001b[39m )\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Convert to pandas\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'RunConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Запуск оценки\n",
    "print(\"Запуск оценки Ragas через модуль src.evaluation...\")\n",
    "df_results = evaluator.run_evaluation(\n",
    "    questions=test_questions,\n",
    "    answers=answers,\n",
    "    contexts=contexts,\n",
    "    ground_truths=ground_truths\n",
    ")\n",
    "\n",
    "print(\"\\nРезультаты оценки:\")\n",
    "display(df_results)\n",
    "\n",
    "# Сохранение результатов\n",
    "evaluator.save_results(df_results, \"data/evaluation_results/evaluation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
